{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7467bcd4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a585c",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are modifications to the original FFN design, while traditionally FFNs will take in all features to be processed, CNNs can restrict the amount of features that are processed at one time through the use of a spatial (or temporal) kernel. \n",
    "\n",
    "For 1D CNNs, for instance audio, you'll apply the same set of CNN filters at each of the timesteps of the audio. CNNs have a kernel size, which allow them to take neighboring datapoints into account when they are applied. Similarly for 2D CNNs, this means applying the same CNN filters to each pixel of the image. 2D kernels will take surrounding pixels (horizontal and vertical) into input as well.\n",
    "\n",
    "CNNs are better able to deal with sequences of data, in the sense that convolutions can allow the network to take information along a sequence into account, and through stacking CNNs with elements like max pooling eventually be able to have access to the entire sequence into context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c6d98",
   "metadata": {},
   "source": [
    "#### Overview of a 1D CNN module.\n",
    "\n",
    "![](images/conv1d.png)\n",
    "\n",
    "Here a 2 channel, kernel-size 3, CNN is applied to an input of batch size 4, channel size 3, and length 4. First padding elements (grey) are added to the input, and we apply the CNN (center) to each of the timesteps of the input. The process shown for our first segment is shown in blue, and is repeated across all timesteps to build our complete output. \n",
    "\n",
    "For normal 1D CNNs all channels in the input are used for a given timestep, and the number of input channels must be specified, this is why our CNN consists of a width of 2 (for output channels), height of 4 (for our input channels), and length of 3 (for our kernel width).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e665b1",
   "metadata": {},
   "source": [
    "# Components of a CNN system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d7d39",
   "metadata": {},
   "source": [
    "## CNN Kernel Size\n",
    "\n",
    "Like FFNs, CNNs have a set of weights that can be updated and used to learn the task at hand. 1D CNNs are applied, similarly to FFNs, across all input features (in the case of a text classification task, this usually means the embedding dimension of your text). Unlike FFNs, however, CNNs have a 'width' to them which corresponds to their kernel-size. For the same type of a problem, a CNN with kernel size, $k$, will have $k \\times$ as many weights as a FFN being applied to the same task, thus it is somewhat more computationally complex. \n",
    "\n",
    "Because kernels are generally centered at a time step, odd number sizes of kernels ($3$, $5$, and $7$ are common values) are used to ensure that the convolution is not lopsided.\n",
    "\n",
    "Let's look at how to setup a basic CNN and experiment with different kernel sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f192313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\n",
      "With a size 3 kernel\n",
      "Input tensor has shape: torch.Size([2, 3, 5])\n",
      "Output tensor has shape: torch.Size([2, 5, 3])\n",
      "With a size 5 kernel\n",
      "Input tensor has shape: torch.Size([2, 3, 5])\n",
      "Output tensor has shape: torch.Size([2, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch, torchtext\n",
    "from torch import nn, Tensor\n",
    "\n",
    "print(\"CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\")\n",
    "input_size = 3\n",
    "number_of_cnn_filters = 5 # number of channels in the output\n",
    "\n",
    "example_cnn_kernel3 = nn.Conv1d(input_size,number_of_cnn_filters,3)\n",
    "x = torch.randn(2,3,5)\n",
    "\n",
    "\n",
    "print(\"With a size 3 kernel\")\n",
    "print(\"Input tensor has shape: {}\".format(x.shape))\n",
    "out = example_cnn_kernel3(x)\n",
    "print(\"Output tensor has shape: {}\".format(out.shape))\n",
    "\n",
    "example_cnn_kernel5 = nn.Conv1d(input_size,number_of_cnn_filters,5)\n",
    "\n",
    "print(\"With a size 5 kernel\")\n",
    "print(\"Input tensor has shape: {}\".format(x.shape))\n",
    "out = example_cnn_kernel5(x)\n",
    "print(\"Output tensor has shape: {}\".format(out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854ff48",
   "metadata": {},
   "source": [
    "In our both cases the Batch dimension (usually written as $N$) doesn't change, but our channel dimension ($C$, and usually corresponding to the hidden $H$ or embedding $E$ dimension in other layers), is set to the number of CNN filters in our layer (1 filter = 1 output channel). The last dimension, our length $L$ dimension, however, decreases depending on our kernel size. (You can experiment with this yourself, what happens if you set the kernel size > the length?)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16086fd1",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "To account for differences in input size, and allow for consistent sizes as data is passed through a CNN model, padding, $0$ valued cells, can be added to the front and the back of the input. This allows a CNN to apply to the very first value in an input sequence, without shortening the output length. For simple models (with odd-number kernel sizes), picking a padding size, $p$, such that $p = \\frac{k - 1}{2}$ can ensure a consistent size through a CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9529c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\n",
      "With a size 5 kernel and padding = 2\n",
      "Input tensor has shape: torch.Size([2, 3, 5])\n",
      "Output tensor has shape: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\")\n",
    "\n",
    "input_size = 3\n",
    "number_of_cnn_filters = 5 # number of channels in the output\n",
    "\n",
    "example_cnn_with_padding = nn.Conv1d(input_size,number_of_cnn_filters,5, padding =2)\n",
    "x = torch.randn(2,3,5)\n",
    "\n",
    "print(\"With a size 5 kernel and padding = 2\")\n",
    "print(\"Input tensor has shape: {}\".format(x.shape))\n",
    "out = example_cnn_with_padding(x)\n",
    "print(\"Output tensor has shape: {}\".format(out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b77f9",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Finally, while a kernel size of the CNN allows for the model to learn spatial or temporal aspects of the input, often time for convenience a layer is added to the model to take a only a portion of 'most useful' samples from one layer of the model to the next. One way to do this is through a *Max Pooling* layer, which chunks up data, and only passes the largest value in each chunk to the next layer. This is an easy way to dramatically decrease the size of the input from one layer to the next, speeding up processing time while further highlighting the most important features in the data. Similarly, *Average Pooling* can be used to take the average of a chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae56851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\n",
      "With a size 5 kernel and padding = 2\n",
      "Input tensor has shape: torch.Size([2, 3, 20])\n",
      "Output tensor from CNN has shape: torch.Size([2, 5, 20])\n",
      "Output tensor from Pooling has shape: torch.Size([2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\")\n",
    "\n",
    "input_size = 3\n",
    "number_of_cnn_filters = 5 # number of channels in the output\n",
    "\n",
    "example_cnn_with_padding = nn.Conv1d(input_size,number_of_cnn_filters,3, padding =1)\n",
    "example_max_pool = nn.MaxPool1d(2)  # Pooling Kernel = 2 means we HALVE the length\n",
    "x = torch.randn(2,3,20)\n",
    "\n",
    "print(\"With a size 5 kernel and padding = 2\")\n",
    "print(\"Input tensor has shape: {}\".format(x.shape))\n",
    "out = example_cnn_with_padding(x)\n",
    "print(\"Output tensor from CNN has shape: {}\".format(out.shape))\n",
    "out = example_max_pool(out)\n",
    "print(\"Output tensor from Pooling has shape: {}\".format(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826996d6",
   "metadata": {},
   "source": [
    "Finally a handy variant of normal max pooling is AdaptivePooling, which can be used to compress an input into a fixed number of outputs, regardless of what the input length is. This is handy at the end of a model to shrink all the timesteps down to a single point (which can then be fed to a linear layer to project to your output classfication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2a6de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\n",
      "With a size 5 kernel and padding = 2\n",
      "Input tensor has shape: torch.Size([2, 3, 20])\n",
      "Output tensor from CNN has shape: torch.Size([2, 5, 20])\n",
      "Output tensor from Pooling has shape: torch.Size([2, 5, 1])\n",
      "Final size after dropping L dimension: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN input/output dimensions:[N,C,L] --> N (Batch), C (Channel), L (Length)\")\n",
    "\n",
    "input_size = 3\n",
    "number_of_cnn_filters = 5 # number of channels in the output\n",
    "\n",
    "example_cnn_with_padding = nn.Conv1d(input_size,number_of_cnn_filters,3, padding =1)\n",
    "example_adaptive_pool = nn.AdaptiveMaxPool1d(1)  # Squish into one timestep, taking highest value for each channel.\n",
    "x = torch.randn(2,3,20)\n",
    "\n",
    "print(\"With a size 5 kernel and padding = 2\")\n",
    "print(\"Input tensor has shape: {}\".format(x.shape))\n",
    "out = example_cnn_with_padding(x)\n",
    "print(\"Output tensor from CNN has shape: {}\".format(out.shape))\n",
    "out = example_adaptive_pool(out)\n",
    "print(\"Output tensor from Pooling has shape: {}\".format(out.shape))\n",
    "\n",
    "out = out.squeeze()\n",
    "print(\"Final size after dropping L dimension: {}\".format(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e23f3",
   "metadata": {},
   "source": [
    "# Classification Task Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d5e8f",
   "metadata": {},
   "source": [
    "With the basics of these CNN layers, we can tackle a slightly more complex challenge compared to the single sentence classification task. In this task, we'll use the CNN to capture meaning across a length of tokens in our input paragraph. We'll be looking at IMDB movie reviews, again classifying into positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbea632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchtext\n",
    "from torch import nn, Tensor\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import dataset, random_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b30c7",
   "metadata": {},
   "source": [
    "Now let's get our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd97763",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train = IMDB(split=\"train\")\n",
    "IMDB_train = to_map_style_dataset(IMDB_train)\n",
    "IMDB_train, IMDB_dev = random_split(IMDB_train, [15000,10000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e538ef",
   "metadata": {},
   "source": [
    "To look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a3e0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 'This is a very funny movie, easy to watch, that entertains you almost all the time. The work of the Director is recognizable and the type of humor is his trademark. The movie is a typical police partners history like lethal weapon, but the jokes and comedy are of Argentinian sort. The twist is that one of them is a psychologist played by Peretti and has to go with detective Diaz (played by Luque) on his assignments while he also assist him (Diaz is troubled because his wife cheated on him). Some of the dialogs are hilarious worldwide: understandable and laughable anywhere. Is very good overall, it would deserved an 8, but I rated 7 because it gets a little down at the end. On a personal remark I must add that is a \"bravo\" for Argentinian Filmmakers, considering the little good is coming lately.')\n"
     ]
    }
   ],
   "source": [
    "for i in IMDB_train:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4f954",
   "metadata": {},
   "source": [
    "Let's get the vocabulary of our train set, that way we can easily assign tokens to IDs and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3a4607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), IMDB_train), min_freq=5, specials=['<unk>','<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108a65c",
   "metadata": {},
   "source": [
    "We want to break the full sentence into individual word tokens, and get them in a format our model can handle, here is a basic way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86338f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any, Union\n",
    "\n",
    "class default_tokenizer(nn.Module):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self, input: Any) -> Any:\n",
    "        if torch.jit.isinstance(input, List[str]):\n",
    "            tokens: List[List[str]] = []\n",
    "            for text in input:\n",
    "                tokens.append(self.tokenizer(text))\n",
    "            return tokens\n",
    "        elif torch.jit.isinstance(input, str):\n",
    "            return self.tokenizer(input)\n",
    "        else:\n",
    "            raise TypeError(\"Input type not supported\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfb4e327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23317"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f08d6",
   "metadata": {},
   "source": [
    "For our text processing, we will use torchtext's Transforms to apply the tokenization and mapping to ID in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10c4cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "processor = T.Sequential(\n",
    "    default_tokenizer(tokenizer),\n",
    "    T.VocabTransform(vocab),\n",
    "    )\n",
    "label_processor = T.LabelToIndex(['pos','neg'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d008484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_processor(_label))\n",
    "        processed_text = torch.tensor(processor(_text))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=vocab.__getitem__(\"<pad>\"))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(list(IMDB_train), batch_size=16, shuffle=True, \n",
    "                              collate_fn=collate_batch)\n",
    "dev_dataloader = DataLoader(list(IMDB_dev), batch_size=16, shuffle=False, \n",
    "                              collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57c49a",
   "metadata": {},
   "source": [
    "Let's look at what a processed example looks like. Note the padding (token ID=1) in our second tensor. While our Labels are just a 1d tensor, our input text is a 2D tensor, with dimensions [L,N] (length corresponding to rows, batch corresponding to columns (so one sentence per column from top to bottom), this is why there are so many 1s towards the bottom rows of the tensor, they are all the padded parts of the short sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a3764aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]), tensor([[  14,  263,  107,  ..., 1755, 1322,   13],\n",
      "        [  52,   72,   23,  ...,   76,    0,   96],\n",
      "        [   2,  115,   69,  ...,    8,   52,    9],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]]))\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18d532",
   "metadata": {},
   "source": [
    "Let's re-use our training code from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bb2a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "def train(loader, model, criterion, optimizer, device):\n",
    "    total_loss = 0.0\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_output, batch_input = batch  #unpack the batch\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss / num_sample\n",
    "\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader, model, device):\n",
    "    all_pred = []\n",
    "    all_label = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # load the current batch\n",
    "            batch_output, batch_input = batch\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "\n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d5d90",
   "metadata": {},
   "source": [
    "Our model definition. I also make some helper functions (PermuteModule and SqueezeModule) to make the forward pass simple. These helper modules just apply their respective functions when they are chained in the ModuleList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce395ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER MODULES\n",
    "class PermuteModule(nn.Module):\n",
    "    ## APPLY PERMUTE WHEN USED IN A MODULELIST\n",
    "    def __init__(self, dims):\n",
    "        super(PermuteModule, self).__init__()\n",
    "        self.dims = dims\n",
    "    def forward(self, x):\n",
    "        return torch.permute(x, self.dims)\n",
    "\n",
    "class SqueezeModule(nn.Module):\n",
    "    ## APPLY SQUEEZE WHEN USED IN A MODULELIST\n",
    "    def __init__(self, dims=None):\n",
    "        super(SqueezeModule, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, output_size, layers, channels, kernel_size, vocab_size, embedding_size, dropout):\n",
    "        super(ConvNet, self).__init__()\n",
    "        '''\n",
    "        output_size: Number of classes in a classification task\n",
    "        layers: How many CNN layers to apply\n",
    "        channels: Number of filters for each CNN to use\n",
    "        kernel_size: width of the 1 Dimensional CNN kernel (each channel will take in this much width when creating an output point)\n",
    "        vocab_size: the number of tokens in our vocabulary\n",
    "        embedding_size: the number of dimensions to represent each token\n",
    "        dropout: regularization probability.\n",
    "        '''        \n",
    "        \n",
    "        self.layers = nn.ModuleList()  # Keep track of our layers\n",
    "        self.nonlinear = nn.ReLU()     # Non-linear activation \n",
    "        self.dropout = nn.Dropout(p=dropout)     #  Dropout helps the network generalize to unseen data\n",
    "        self.pooling = nn.MaxPool1d(kernel_size=2)   # Our pooling operation, here we divide the length in half each time.\n",
    "        self.layers.append(PermuteModule((1,0)))\n",
    "        self.layers.append(nn.Embedding(vocab_size, embedding_size, padding_idx=vocab.__getitem__(\"<pad>\")))  # Convert input vocab indices to vectors (doesn't compress L dimension like EmbeddingBag)\n",
    "        self.layers.append(PermuteModule((0,2,1)))\n",
    "\n",
    "        self.layers.append(nn.Conv1d(embedding_size, channels, kernel_size))   #  Basic CNN \n",
    "        self.layers.append(self.nonlinear)\n",
    "        for layer in range(layers):    # For each layer, add a Linear, nonlinear activation, and dropout.\n",
    "            self.layers.append(nn.Conv1d(channels, channels, kernel_size))\n",
    "            self.layers.append(self.nonlinear)\n",
    "            self.layers.append(self.pooling)\n",
    "            self.layers.append(self.dropout)\n",
    "        self.layers.append(nn.AdaptiveMaxPool1d(1))   # Collapse the time dimension\n",
    "        self.layers.append(SqueezeModule())   # get rid of collapsed dimension\n",
    "        self.layers.append(nn.Linear(channels, output_size))  # Project to output size\n",
    "        self.layers.append(nn.Softmax(dim=-1))   # Softmax normalizes vectors so they can represent probabilities\n",
    "    \n",
    "    def forward(self, x):    # out 'foward' pass, since we put everything into a ModuleList, we just iterate it.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ba8fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(2, 2, 128, 3, len(vocab), 128, .3)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_epochs = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbca63",
   "metadata": {},
   "source": [
    "Before we train, let's look a little bit more at our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7899bc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire model looks like this: \n",
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): PermuteModule()\n",
      "    (1): Embedding(23317, 128, padding_idx=1)\n",
      "    (2): PermuteModule()\n",
      "    (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Dropout(p=0.3, inplace=False)\n",
      "    (13): AdaptiveMaxPool1d(output_size=1)\n",
      "    (14): SqueezeModule()\n",
      "    (15): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (16): Softmax(dim=-1)\n",
      "  )\n",
      "  (nonlinear): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "If we just wanted to look at the ModuleList, we could just print it out as well (these are the sequence of layers) \n",
      "ModuleList(\n",
      "  (0): PermuteModule()\n",
      "  (1): Embedding(23317, 128, padding_idx=1)\n",
      "  (2): PermuteModule()\n",
      "  (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (4): ReLU()\n",
      "  (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (6): ReLU()\n",
      "  (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Dropout(p=0.3, inplace=False)\n",
      "  (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (10): ReLU()\n",
      "  (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (12): Dropout(p=0.3, inplace=False)\n",
      "  (13): AdaptiveMaxPool1d(output_size=1)\n",
      "  (14): SqueezeModule()\n",
      "  (15): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (16): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"The entire model looks like this: \")\n",
    "print(model)\n",
    "print(\"If we just wanted to look at the ModuleList, we could just print it out as well (these are the sequence of layers) \")\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8d42e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 results: Loss 0.04337153381506602, Train Acc: 0.5020666666666667, Val Acc: 0.5033\n",
      "Epoch 2 results: Loss 0.043280045413970944, Train Acc: 0.5516666666666666, Val Acc: 0.5586\n",
      "Epoch 3 results: Loss 0.04310861246188482, Train Acc: 0.5213333333333333, Val Acc: 0.5224\n",
      "Epoch 4 results: Loss 0.04258451509078344, Train Acc: 0.592, Val Acc: 0.593\n",
      "Epoch 5 results: Loss 0.041469856758912405, Train Acc: 0.6276666666666667, Val Acc: 0.6254\n",
      "Epoch 6 results: Loss 0.03934925354917844, Train Acc: 0.709, Val Acc: 0.7011\n",
      "Epoch 7 results: Loss 0.03698840180635452, Train Acc: 0.7215333333333334, Val Acc: 0.7134\n",
      "Epoch 8 results: Loss 0.035994927604993184, Train Acc: 0.7342666666666666, Val Acc: 0.7187\n",
      "Epoch 9 results: Loss 0.03508005459507307, Train Acc: 0.7388666666666667, Val Acc: 0.7189\n",
      "Epoch 10 results: Loss 0.03448299227754275, Train Acc: 0.7713333333333333, Val Acc: 0.7372\n",
      "Epoch 11 results: Loss 0.033710312559207284, Train Acc: 0.7625333333333333, Val Acc: 0.7306\n",
      "Epoch 12 results: Loss 0.03291306788126628, Train Acc: 0.7746, Val Acc: 0.7385\n",
      "Epoch 13 results: Loss 0.032266579729318616, Train Acc: 0.8062, Val Acc: 0.7562\n",
      "Epoch 14 results: Loss 0.031673758004109064, Train Acc: 0.8020666666666667, Val Acc: 0.7489\n",
      "Epoch 15 results: Loss 0.030933866383632023, Train Acc: 0.8397333333333333, Val Acc: 0.7718\n",
      "Epoch 16 results: Loss 0.03025021217465401, Train Acc: 0.8405333333333334, Val Acc: 0.7738\n",
      "Epoch 17 results: Loss 0.02966973064740499, Train Acc: 0.8470666666666666, Val Acc: 0.7744\n",
      "Epoch 18 results: Loss 0.02906977247595787, Train Acc: 0.7865333333333333, Val Acc: 0.7372\n",
      "Epoch 19 results: Loss 0.028422878859440486, Train Acc: 0.816, Val Acc: 0.7483\n",
      "Epoch 20 results: Loss 0.02791788649360339, Train Acc: 0.8749333333333333, Val Acc: 0.791\n",
      "Epoch 21 results: Loss 0.027454456239938738, Train Acc: 0.7905333333333333, Val Acc: 0.7299\n",
      "Epoch 22 results: Loss 0.026840595181783042, Train Acc: 0.9094, Val Acc: 0.802\n",
      "Epoch 23 results: Loss 0.026220176088809968, Train Acc: 0.9069333333333334, Val Acc: 0.7975\n",
      "Epoch 24 results: Loss 0.025845756687720618, Train Acc: 0.8784, Val Acc: 0.7792\n",
      "Epoch 25 results: Loss 0.025468502058585484, Train Acc: 0.9257333333333333, Val Acc: 0.8075\n",
      "Epoch 26 results: Loss 0.0250710243721803, Train Acc: 0.928, Val Acc: 0.8014\n",
      "Epoch 27 results: Loss 0.024695224342743554, Train Acc: 0.9363333333333334, Val Acc: 0.8128\n",
      "Epoch 28 results: Loss 0.02450626335342725, Train Acc: 0.9403333333333334, Val Acc: 0.8105\n",
      "Epoch 29 results: Loss 0.02422114301721255, Train Acc: 0.9253333333333333, Val Acc: 0.7943\n",
      "Epoch 30 results: Loss 0.023846220940351485, Train Acc: 0.9442, Val Acc: 0.8077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {\"epoch\": [], \"loss\" : [], \"train acc\" : [], \"val acc\" :[]}\n",
    "for i in range(max_epochs):\n",
    "    loss = train(train_dataloader, model, criteria, optimizer, device)\n",
    "    train_acc = evaluate(train_dataloader, model, device)\n",
    "    val_acc = evaluate(dev_dataloader, model, device)\n",
    "    results[\"epoch\"].append(i)\n",
    "    results[\"loss\"].append(loss)\n",
    "    results['train acc'].append(train_acc)\n",
    "    results['val acc'].append(val_acc)\n",
    "    \n",
    "    print(\"Epoch {} results: Loss {}, Train Acc: {}, Val Acc: {}\".format(i+1, loss, train_acc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "950cc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14844f77",
   "metadata": {},
   "source": [
    "Let's plot our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9d2d2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "loss_line, = ax1.plot(results[\"epoch\"],results[\"loss\"], linewidth=1.0, color=color, label= \"loss\")\n",
    "ax1.set(xlim=(0, 30), xticks=np.arange(0, 30,5))                               \n",
    "ax1.set_ylabel('loss', color=color)                             \n",
    "ax1.set_xlabel('epochs')\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "\n",
    "train_line, = ax2.plot(results[\"epoch\"],results[\"train acc\"], linewidth=1.0, label='train acc')\n",
    "val_line, = ax2.plot(results[\"epoch\"],results[\"val acc\"], linewidth=1.0, label='val acc')\n",
    "ax2.set(xlim=(0, 30), xticks=np.arange(0, 30,5),\n",
    "       ylim=(0, 1))\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "p = [val_line, train_line, loss_line]\n",
    "ax2.legend(p, [p_.get_label() for p_ in p])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57072b59",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
