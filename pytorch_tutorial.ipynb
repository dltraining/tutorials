{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*jcZLpgh3gppeFFgcpFSP0w.jpeg\" alt=\"PyTorch\" title=\"PyTorch\" style=\"width: 200px;\"/>\n",
    "\n",
    "PyTorch is a Python based tool for scientific computing that provides three main features:\n",
    "- An n-dimensional Tensor, which is similar to numpy but can run on GPUs\n",
    "- Easily build big computational graphs for deep learning\n",
    "- Automatic differentiation for computing gradients for neural networks\n",
    "\n",
    "You can install PyTorch from: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's Tensor\n",
    "\n",
    "Tensor is an n-dimensional array, which is a generalization of a matrix that can be indexed in more than 2 dimensions. Tensor is similar to numpy that most of the operations in numpy object can be performed on a tensor object. However, tensor object benefits from strong GPU acceleration while numpy does not. All computations in deep learning are performed on tensors. Tensors also store optional information such as gradient and bookkeeping for computational graph. \n",
    "\n",
    "\n",
    "###  Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized tensor of size 5x3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6839, 0.3716, 0.3839],\n",
      "        [0.2629, 0.0449, 0.2231],\n",
      "        [0.1178, 0.9471, 0.7022],\n",
      "        [0.6992, 0.4817, 0.5664],\n",
      "        [0.9368, 0.7465, 0.2518]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3) # a tensor filled with random numbers from a uniform distribution on the interval [0,1)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor filled zeros and of data type (dtype) long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5000,  3.0000],\n",
      "        [ 3.2000, 13.0000],\n",
      "        [ 6.9000, 23.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[5.5, 3], [3.2, 13], [6.9, 23]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor based on existing tensor: (reusing properties of input tensor like dtype by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5326, -1.8590],\n",
      "        [-0.0550, -0.1841],\n",
      "        [-0.6633, -0.8143]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn_like(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y.size())  # you can also use y.shape()\n",
    "print(y.size(1)) # get the number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a 3D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.5000,  3.0000],\n",
      "         [ 3.2000, 13.0000],\n",
      "         [ 6.9000, 23.0000]],\n",
      "\n",
      "        [[ 2.1000,  3.3000],\n",
      "         [ 1.8000,  2.0000],\n",
      "         [ 5.2000, 20.0000]]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([ [[5.5, 3], [3.2, 13], [6.9, 23]], [[2.1, 3.3], [1.8, 2], [5.2, 20]] ])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a 3D tensor anyway? Think about it like this. If you have a vector, indexing into the vector gives you a scalar. If you have a matrix, indexing into the matrix gives you a vector. If you have a 3D tensor, then indexing into the tensor gives you a matrix!\n",
    "\n",
    "The size of the 3D tensor will be 2x3x2 (in this example, 3D tensor is a collection of two 3x2 matrices). Let's print the size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1000,  3.3000],\n",
      "        [ 1.8000,  2.0000],\n",
      "        [ 5.2000, 20.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(z[1]) # accesses the second dimension (matrix) whose size will be 3x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "print(z[1][1]) # accesses the second dimension of the second dimension (vector) whose size will be 1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(z[1][1][1]) # accesses the second dimension of the second dimension of the second dimension (scalar) whose size will be 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(z.dtype) # prints data type of tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default data type of a tensor is Float. If you want an integer tensor, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "it = torch.tensor([3, 4], dtype=torch.int)\n",
    "print(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Operations on a Tensor\n",
    "\n",
    "There are multiple syntaxes for operations. Let us take a look at the addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3471e-01, 2.5028e-04, 7.6348e-01],\n",
      "        [9.7835e-01, 4.7258e-01, 9.5781e-02],\n",
      "        [3.0613e-01, 4.6486e-01, 1.7439e-01],\n",
      "        [4.1998e-01, 1.7716e-03, 7.4684e-01],\n",
      "        [5.6333e-01, 6.7918e-01, 7.9335e-01]])\n",
      "tensor([[0.2797, 0.3715, 0.3162],\n",
      "        [0.3177, 0.1429, 0.6317],\n",
      "        [0.9134, 0.8784, 0.2730],\n",
      "        [0.8035, 0.0474, 0.7807],\n",
      "        [0.5321, 0.1873, 0.2090]])\n"
     ]
    }
   ],
   "source": [
    "# let's create two tensors\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "# let's print those two tensors\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7144, 0.3717, 1.0797],\n",
      "        [1.2960, 0.6155, 0.7274],\n",
      "        [1.2195, 1.3433, 0.4473],\n",
      "        [1.2235, 0.0491, 1.5276],\n",
      "        [1.0954, 0.8665, 1.0024]])\n",
      "tensor([[0.7144, 0.3717, 1.0797],\n",
      "        [1.2960, 0.6155, 0.7274],\n",
      "        [1.2195, 1.3433, 0.4473],\n",
      "        [1.2235, 0.0491, 1.5276],\n",
      "        [1.0954, 0.8665, 1.0024]])\n"
     ]
    }
   ],
   "source": [
    "# let's add them in two ways\n",
    "print(x + y) # method 1\n",
    "print(torch.add(x, y)) # method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7144, 0.3717, 1.0797],\n",
       "        [1.2960, 0.6155, 0.7274],\n",
       "        [1.2195, 1.3433, 0.4473],\n",
       "        [1.2235, 0.0491, 1.5276],\n",
       "        [1.0954, 0.8665, 1.0024]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x) # adds x to y (in-place) y := y + x  (method 3) or alternatively we can do y = torch.add(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operation that mutates a tensor in-place is post-fixed with an ``_``. For example: ``x.copy_(y)``, ``x.t_()`` (transpose), will change x.\n",
    "\n",
    "We can use standard NumPy-like indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3471e-01, 2.5028e-04, 7.6348e-01],\n",
      "        [9.7835e-01, 4.7258e-01, 9.5781e-02],\n",
      "        [3.0613e-01, 4.6486e-01, 1.7439e-01],\n",
      "        [4.1998e-01, 1.7716e-03, 7.4684e-01],\n",
      "        [5.6333e-01, 6.7918e-01, 7.9335e-01]])\n",
      "tensor([2.5028e-04, 4.7258e-01, 4.6486e-01, 1.7716e-03, 6.7918e-01])\n"
     ]
    }
   ],
   "source": [
    "print(x) # prints x whose size is 5x3\n",
    "print(x[:, 1]) # prints the second column of tensor whose size is 1x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us resize/reshape tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# let's print x's size\n",
    "print(x.size()) #  5x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3471e-01, 2.5028e-04, 7.6348e-01, 9.7835e-01, 4.7258e-01, 9.5781e-02,\n",
      "        3.0613e-01, 4.6486e-01, 1.7439e-01, 4.1998e-01, 1.7716e-03, 7.4684e-01,\n",
      "        5.6333e-01, 6.7918e-01, 7.9335e-01])\n",
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "# let's reshape x to a flat array\n",
    "print(x.view(15)) # print reshaped tensor\n",
    "print(x.view(15).size()) # print size of the reshaped tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us multiply two tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create two tensors\n",
    "a = torch.randn(4, 1) \n",
    "b = torch.randn(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3922,  0.0585,  0.4208,  0.0853],\n",
       "        [ 3.5755, -0.5333, -3.8356, -0.7776],\n",
       "        [ 0.1160, -0.0173, -0.1245, -0.0252],\n",
       "        [ 0.8985, -0.1340, -0.9639, -0.1954]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's multiply each other: a x b \n",
    "torch.mul(a, b) # (4 x 1) x (1 x 4) = (4 x 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the mean of a tensor in one particular dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7573,  0.2316,  0.3929, -0.4683, -1.4706],\n",
      "        [ 0.8319,  0.1011,  0.1805,  1.8710,  1.6045],\n",
      "        [-0.3907,  1.5070,  0.2966,  0.4689, -2.4953]])\n"
     ]
    }
   ],
   "source": [
    "# let's create a tensor\n",
    "a = torch.randn(3, 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4144,  0.9178, -0.1227])\n"
     ]
    }
   ],
   "source": [
    "# let's perform mean of the tensor over columns\n",
    "print(torch.mean(a, 1)) # reduce over columns (1) results in 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1054,  0.6132,  0.2900,  0.6239, -0.7871])\n"
     ]
    }
   ],
   "source": [
    "# let's perform mean of the tensor over rows\n",
    "print(torch.mean(a, 0)) # reduce over rows (0) results in 1x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the max of a tensor in one particular dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0470,  1.2275, -0.9094],\n",
      "        [ 0.3157, -1.0225,  0.4018],\n",
      "        [ 0.0237, -1.3291, -0.2868],\n",
      "        [-0.0337,  1.7147,  1.0138]])\n"
     ]
    }
   ],
   "source": [
    "# let's create a tensor\n",
    "a = torch.randn(4, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2275, 0.4018, 0.0237, 1.7147])\n",
      "tensor([1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# let's identify maximum in each row\n",
    "values, indices = torch.max(a, 1) \n",
    "print(values) # values is the maximum value of each row of the input tensor. (1x4)\n",
    "print(indices) # indices is the index location of each maximum value found (argmax) (1x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the list of supported Tensor functions here: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "### NumPy Bridge \n",
    "We can easily convert a Torch tensor to a numpy array and vice versa.\n",
    "\n",
    "Let us convert a Torch Tensor to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# let's create a tensor\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# let's convert that tensor to numpy\n",
    "b = a.numpy() # converts Tensor to NumPy with a,b pointing to same memory locations\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# manipulate 'a' (changes will reflect in 'b')\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert a NumPy array to a Torch Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# let's create a numpy array\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let's convert to tensor\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# manipulate 'a' (changes will reflect in 'b')\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "\n",
    "We can use ``.to`` function to move Tensors onto any device. Generally, we move tensors to GPUs to accelerate the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")          # a CUDA device object\n",
    "  y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "  x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "  z = x + y\n",
    "  print(z)\n",
    "  print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph and Automatic Differentiation\n",
    "\n",
    "Computation graph is an essential concept for efficient deep learning programming, because it allows you to not explicitly write the back propagation gradients yourselves. A computation graph is simply a specification of how your data is combined to give you the output. Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "Let us see this in more simple terms with some examples.\n",
    "\n",
    "Besides keeping track of size, data type and other things, Tensors can also keep track of how it was created. If you set its attribute ``.requires_grad`` as ``True``, it starts to track all operations on it.\n",
    "\n",
    "Let us see some example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([5., 1., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create two tensors\n",
    "a = torch.tensor([1., 2., 3], requires_grad=True) \n",
    "b = torch.tensor([5., 1., 4], requires_grad=True) \n",
    "print(a) # 1x3\n",
    "print(b) # 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 3., 7.])\n"
     ]
    }
   ],
   "source": [
    "# add those tensors\n",
    "c = a + b # 1x3\n",
    "print(c.data) # 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x10bb80c18>\n"
     ]
    }
   ],
   "source": [
    "# but c knows something extra.\n",
    "print(c.grad_fn) # c knows that it was a result of addition of two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x10bb86358>\n"
     ]
    }
   ],
   "source": [
    "# sum all entries in c\n",
    "d = c.sum() # 1x1\n",
    "print(d)\n",
    "print(d.grad_fn) # d knows that it was a sum of all elements in a single tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``c`` knows that it was a result of addition of two tensors while ``d`` knows that it was a sum of all elements in a single tensor. Thus, a computation graph is simply a specification of how your data is combined to give you the output. You can imagine this computational graph as below:\n",
    "<img src=\"images/sl1_pytorch_cgraph_sum.jpg\" alt=\"computational graph example\" title=\"Example Computational Graph\" />\n",
    "\n",
    "Once we define the computational graph, we can call ``.backward()`` and have all the gradients computed automatically. The gradient for a tensor will be accumulated into ``.grad`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "d.backward()\n",
    "print(a.grad) # 1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can trust us that these gradients are correct. PyTorch lets you define arbitrary computation graph that is made up of tensor (``torch.tensor``) and modules (off-the-shelf layers from ``torch.nn`` and custom layers/models). \n",
    "\n",
    "Important Note: If you run the above block multiple times, the gradient will increment. That is because Pytorch accumulates the gradient into the ``.grad`` property, since for many models this is very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearities, Nonlinearities and Loss functions\n",
    "\n",
    "A deep learning model is typically composed of linearities (affine transformation) and nonlinearities in a clever way. The nonlinearities makes the deep learning models powerful. The last node of the computational graph is typically a loss function (or objective function), which measure how far away the model prediction is from the actual target. PyTorch has most of the commonly used linearities, nonlinearities and loss functions already inbuilt into the library. Adding them to your computational graph is straightforward as we will see now.\n",
    "\n",
    "### Linearities\n",
    "Affine transformation is the commonly used linearity, which is a function $f(x)$ where $f(x) = A x + b$\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be learned here are $A$ and $b$. Often, $b$ is refered to as the bias term.\n",
    "\n",
    "Let us transform a sample data using affine transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# let's define a affine transformation based linearity layer\n",
    "linear_layer = torch.nn.Linear(5, 3) # maps from R^5 to R^3, parameters A, b\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the parameters: A matrix and b vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4148,  0.0942, -0.3873, -0.2727,  0.1664],\n",
      "        [ 0.0055, -0.3496,  0.2675,  0.3780, -0.4020],\n",
      "        [ 0.1789,  0.3826, -0.1834,  0.3528, -0.4270]])\n",
      "tensor([0.4166, 0.3994, 0.2847])\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight.data) # prints A  3x5\n",
    "print(linear_layer.bias.data) # prints b  1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create some data and pass it to the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create data 'x'\n",
    "x = torch.randn(1, 5) # data is 1x5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4444,  0.7414, -0.2102]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# let's compute f(x) by passing x to the linear layer\n",
    "transformed_output = linear_layer(x)\n",
    "print(transformed_output)\n",
    "# print(linear_layer(torch.randn(1, 6))) # error: size mismatch, m1: [1 x 6], m2: [5 x 3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look into the other linear layers here: https://pytorch.org/docs/stable/nn.html#linear-layers\n",
    "\n",
    "### Nonlinearities\n",
    "Nonlinearities lets you build powerful deep learning models. For example, sigmoid nonlinearity squashes the input to be between 0 and 1. Sigmoid nonlinearity is defined by $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$. \n",
    "\n",
    "Let us see an example for using sigmoid nonlinearity on sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define sigmoid layer\n",
    "sigmoid_layer = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8064,  0.1351,  0.6048,  0.6664,  0.7107]])\n"
     ]
    }
   ],
   "source": [
    "# let's define x\n",
    "x = torch.randn(1, 5) # data is 1x5.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1411, 0.5337, 0.6468, 0.6607, 0.6706]])\n"
     ]
    }
   ],
   "source": [
    "# let's pass x to sigmoid layer\n",
    "sigmoid_out = sigmoid_layer(x) # applies sigmoid element-wise results in 1x5 (doesn't change dimension)\n",
    "print(sigmoid_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used nonlinearity is softmax function, which rescales an n-dimensional input Tensor so that the elements of the n-dimensional output Tensor lie in the range $[0,1]$ and sum to 1. \n",
    "\n",
    "The softmax function is defined as, $softmax(x_i) = \\frac{\\exp{(x_i)}}{\\sum_j{\\exp{(x_j)}}}$\n",
    "\n",
    "Let us see an example for using softmax nonlinearity on sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a softmax layer\n",
    "softmax_layer = torch.nn.Softmax(dim=1) # row-wise softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6029, -0.6874, -1.2841, -1.0747,  0.9904]])\n"
     ]
    }
   ],
   "source": [
    "# let's create x\n",
    "x = torch.randn(1, 5) # data is 1x5.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pass x to the softmax_layer\n",
    "softmax_out = softmax_layer(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1255, 0.1153, 0.0635, 0.0783, 0.6174]])\n"
     ]
    }
   ],
   "source": [
    "# lets print softmax output\n",
    "print(softmax_out) # each entry is between 0 to 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# softmax output sums to 1.0\n",
    "print(softmax_out[0].sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the other nonlinear layers here: https://pytorch.org/docs/stable/nn.html#non-linear-activations-other\n",
    "\n",
    "### Loss function\n",
    "A loss function takes the (model prediction, target) pair of inputs, and computes a value that estimates how far away the model prediction is from the target.\n",
    "\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error between the model prediction ($\\hat{y}_i$) and the target ($y_i$). Mean Squared error can be written as, $(\\hat{y}_i - y_i)^{2}$\n",
    "\n",
    "Let us see an example for using MSELoss on output from Linear+softmax model and (randomly sampled) target (usually target is annotated by human but we create it synthetically in this tutorial for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0261, -0.0251,  0.2468]])\n"
     ]
    }
   ],
   "source": [
    "# create data (input, target)\n",
    "data_input = torch.randn(1,3) # 1 example, 3 input features\n",
    "data_output = torch.randn(1,3) # 1 example, 3 target label\n",
    "print(data_output) # 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear and softmax layer\n",
    "linear_layer = torch.nn.Linear(3, 3)\n",
    "softmax_layer = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4277, 0.1236, 0.4487]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass the data_input through the model (computational graph)\n",
    "transformed_output = linear_layer(data_input) # maps from R^3 to R^1, parameters A, b i.e. maps 1x3 to 1x3\n",
    "model_output = softmax_layer(transformed_output)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0896, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# compute the MSELoss\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(model_output, data_output) \n",
    "print(loss) # the MSE loss of 1 individual example  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine the above computational graph to be like this:\n",
    "\n",
    "<img src=\"images/sl1_pytorch_softmax.jpg\" alt=\"computational graph example\" title=\"Example Computational Graph\" />\n",
    "\n",
    "You can look at the other loss layers here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "## Optimization\n",
    "\n",
    "So far, we know:\n",
    "- how to define an arbitrary computation graph (model) with linearities, nonlinearities\n",
    "- how to add loss function to our model to measure the quality of models' predictions\n",
    "- how to use ``backward`` function to compute the gradients\n",
    "\n",
    "The only remaining piece of your PyTorch model is how to update (or learn) the weights (e.g., parameters $A, b$ of the linear layer). The commonly used optimization algorithm for neural networks is gradient descent (GD), which first randomly initializes the weight and changes the weight based on the update rule: $\\theta^{(t+1)} = \\theta^{(t)} - \\frac{1}{n} \\eta \\nabla_\\theta{L(\\theta)} $, where $\\eta$, $\\theta$ and $L(\\theta)$ corresponds to the learning rate (or step size), the parameters of the model (e.g., $A$ and $b$ put together) and the loss function over the parameters. $\\nabla_\\theta{L(\\theta)}$ correspond to the gradient which is calculated (resides in ``grads`` attribute of the tensor) when you call ``backward`` function. Therefore, the weight at the current step is equivalent to the weight at the previous step subtracted from multiplying the gradient (w.r.t weight in previous step) with the learning rate scaled by the number of examples ($n$) for this update.\n",
    "\n",
    "Let us see an example which updates the weight of our previous Linear+Softmax model with MSELoss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1264,  0.0615,  0.1955],\n",
      "        [ 0.0292, -0.1180,  0.5301],\n",
      "        [-0.4985, -0.4142,  0.0642]])\n"
     ]
    }
   ],
   "source": [
    "# get a reference to the weights (or parameters)\n",
    "model_weights = linear_layer.weight.data # note other layers in the previous graph do not have parameters\n",
    "print(model_weights) # prints model weights before GD update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0540,  0.0602, -0.0344],\n",
      "        [ 0.0160, -0.0178,  0.0102],\n",
      "        [ 0.0380, -0.0424,  0.0242]])\n"
     ]
    }
   ],
   "source": [
    "# compute the gradients ($\\nabla_\\theta{L(\\theta)}$)\n",
    "# the whole graph is differentiated w.r.t. the loss, \n",
    "# and all Tensors in the graph that has requires_grad=True \n",
    "# will have their .grad Tensor accumulated with the gradient.\n",
    "loss.backward()\n",
    "print(linear_layer.weight.grad) # prints the gradient w.r.t each parameter for the given data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1210,  0.0555,  0.1989],\n",
      "        [ 0.0276, -0.1162,  0.5290],\n",
      "        [-0.5023, -0.4100,  0.0617]])\n"
     ]
    }
   ],
   "source": [
    "# make the GD update\n",
    "model_weights = model_weights - (1/1.0) * 0.1 * linear_layer.weight.grad # 0.1 is the learning rate, 1.0 is the no. of examples for this update\n",
    "print(model_weights) # prints model weights after GD update which is 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, a GD update using only 1 training example is termed as stochastic gradient descent (SGD). If GD update uses multiple training examples (our update code actually uses 1 training example), then the optimization algorithm is termed as mini-batch gradient descent. A mini-batch gradient descent algorithm typically runs for several passes (or epochs) over your training data and at each pass, it grabs a mini-batch of training examples to perform the update. The size of the mini-batch and the learning rate are hyperparameters to be selected based on the performance of our model on the examples held out from the training data (or validation set).\n",
    "\n",
    "You can look at the overview of GD optimization algorithms here: https://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ctrl_transformers)",
   "language": "python",
   "name": "ctrl_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
