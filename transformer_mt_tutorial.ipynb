{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqDvmzPtY3j_"
   },
   "source": [
    "## Transformer for Machine Translation\n",
    "\n",
    "<img src=\"https://mostafadehghani.com/wp-content/uploads/2021/04/transformer_arch-1024x604.png\" alt=\"Transformer\" title=\"Transformer\" style=\"width: 650px;\"/>\n",
    "\n",
    "Picture Courtesy: [Tay et al., 2020](https://arxiv.org/pdf/2009.06732.pdf)\n",
    "\n",
    "\n",
    "### Transformer mode\n",
    "\n",
    "It is important to note the differences in the mode of usage of the Transformer block. Transformers can primarily be used in three ways, namely:\n",
    "- encoder-only (e.g., for classification), \n",
    "- decoder-only (e.g., for language modeling), \n",
    "- encoder-decoder (e.g., for machine translation, which is our focus)\n",
    "\n",
    "In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, the encoder and encoder-decoder cross attention can afford to be non-causal but the decoder self-attention must be causal. The ability to support causal auto-regressive decoding is required when designing efficient self-attention mechanisms since it can be a limiting factor in many applications.\n",
    "\n",
    "This notebook was tested in a [google colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJtlswApZQyd",
    "outputId": "13783848-8d35-45b8-c754-607e4fe9bcbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7vLl5mFKCgO"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n76MDk3BZSPI",
    "outputId": "caa57b5f-e03b-4c83-9205-d65bd1dae95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "# set the pseudo-random generator\n",
    "manual_seed = 77\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEM1Ah413yPC"
   },
   "source": [
    "### Preparing Data\n",
    "\n",
    "***Define tokenizers:***\n",
    "we create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string.\n",
    "\n",
    "spaCy has model for each language (\"fr\" for French and \"en\" for English) which need to be loaded so we can access the tokenizer of each model.\n",
    "\n",
    "***Note***: the models must first be downloaded using the following on the command line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgAzBFPa34OA",
    "outputId": "d06dfafb-ee54-48bc-c7c3-60e46e198885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 4.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Collecting fr_core_news_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.7 MB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.21.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.10.8)\n",
      "Building wheels for collected packages: fr-core-news-sm\n",
      "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-py3-none-any.whl size=14727025 sha256=d1545f83306096363105ab98c087d3b2954d55c6e2b031aa24dc5282c7a1bad8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rax20ehi/wheels/c9/a6/ea/0778337c34660027ee67ef3a91fb9d3600b76777a912ea1c24\n",
      "Successfully built fr-core-news-sm\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w7Vo9aLW4JJI"
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_fr = fr_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7MRaUm64Mn6"
   },
   "source": [
    "Next, we create the tokenizer functions. These can be passed to TorchText and will take in the sentence as a string and return the sentence as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YenFu4bl4PpT"
   },
   "outputs": [],
   "source": [
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bajlvdbS4SM8"
   },
   "source": [
    "`TorchText`'s Fields handle how data should be processed. You can read all of the possible arguments [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61).\n",
    "\n",
    "We set the tokenize argument to the corresponding tokenization function for each, with French being the `SRC` (source) field and English being the `TRG` (target) field. The field also appends the \"start of sequence\" (\\<sos\\>) and \"end of sequence\" (\\<eos\\>) tokens via the `init_token` and `eos_token` arguments, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IljSJYO34U1R"
   },
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize = tokenize_fr, \n",
    "            # init_token = '<sos>', # since initial encoder hidden state is always set to zero, the network can figure out that the time step is 0 and this token is optional\n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TRG = data.Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV-Fmhgt4XQ8"
   },
   "source": [
    "Next, we load the train, validation and test data.\n",
    "\n",
    "The dataset we'll be using is the [Multi30k](https://github.com/multi30k/dataset) dataset. This is a dataset with ~30,000 parallel English, French and German sentences. You can find more information in [WMT18](http://www.statmt.org/wmt18/multimodal-task.html). This corpus was officially split to Training (29,000 sentences), Validation (1,014 sentences), and multiple Test sets. We provide Test 2016 (1,000 sentences). \n",
    "\n",
    "The raw dataset is extracted to three `.tsv` files. Each file includes two column, 'English' and 'French'. We use `torchtext.legacy.data.TabularDataset` to load these tsv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X4Sdsehn4a12"
   },
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "    path='./drive/MyDrive/Colab Notebooks/eng-fre/', train='train_eng_fre.tsv',validation='val_eng_fre.tsv', test='test_eng_fre.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('TRG', TRG), ('SRC', SRC)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9p0puCR4e9a"
   },
   "source": [
    "We can double check that we've loaded the right number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW20Vuu04gtG",
    "outputId": "b04b4bb4-fd4b-47c7-fe66-3d77bd13e7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIgiKHjM4lru"
   },
   "source": [
    "We can also print out an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwaIIrt-4oB6",
    "outputId": "cea0b52f-f630-41d3-df94-b8a9593b1d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'SRC': ['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1eTer8o4qTu",
    "outputId": "1781d1ca-e342-4057-dc67-fa9f48008682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'], 'SRC': ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(val.examples[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w_Ziwfv4tV2"
   },
   "source": [
    "Next, we'll build the vocabulary for the source and target languages. \n",
    "\n",
    "The vocabulary is used to associate each unique token with an index and this is used to build a one-hot encoding for each token. The vocabularies of the source and target languages have some minimal overlap.\n",
    "\n",
    "Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
    "\n",
    "It is important to note that your vocabulary should only be built from the `training set` and not the `validation/test set`. This prevents **\"information leakage\"** into your model, giving you artifically inflated validation/test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "59r6mL7J4y4a"
   },
   "outputs": [],
   "source": [
    "TRG.build_vocab(train,min_freq=2)\n",
    "SRC.build_vocab(train,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z49Fmd4342ei",
    "outputId": "6ca299cf-0b2b-4095-fa94-8fe7873ecfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 6461\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (fr) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph-VDsasu-12"
   },
   "source": [
    "`TRG.vocab.stoi` is the dictionary of word to index. For example, the index of `<pad>` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCNG_y5Hu2Qz",
    "outputId": "64921265-1273-4a4a-872a-dae681f32a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP2tGhlB4_N6"
   },
   "source": [
    "The final step of preparing the data is to create the `iterators` to generate batches. These can be iterated on to return a batch of data. The text of both source and target text will be converted to two sequence of corresponding indexes, using the vocabularies.\n",
    "\n",
    "\n",
    "We also need to define a `torch.device`. This indicate whether the input `tensors` should be sent to `GPU` or not. We already defined the `device` variable before. \n",
    "\n",
    "Finally, the output of the iterator will be `padded`. \n",
    "\n",
    "We use a `BucketIterator` to creates batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnxNOrTPvE7O",
    "outputId": "9a52af1e-179c-4b19-e7f2-d322894aee44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CdgS6bBEvK1d"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(16, 256, 256),device = device,\n",
    "    sort_key=lambda x: len(x.SRC), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFif-pZOvQhx"
   },
   "source": [
    "Each batch will include two tensors: tensor of source language and tensor of target language. The size of each tensor is **[max_length, batch_size]**. Each example is already padded within batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzicRD44vNSv",
    "outputId": "386aad47-c73c-4e34-89eb-5d577e5c8d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size of source language: torch.Size([20, 16])\n",
      "tensor size of target language: torch.Size([21, 16])\n",
      "the tensor of first example in target language: tensor([   2, 1487, 1332,    0,  245,   18,  931,    4,    9,    5,    3,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# batch example of training data\n",
    "for batch in train_iter:\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    print('the tensor of first example in target language:', trg[:,0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd-1h9chxSzo"
   },
   "source": [
    "We save our Fields for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Hoe5kPCfxWi4"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./drive/MyDrive/Colab Notebooks/ckpt_attn/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"./drive/MyDrive/Colab Notebooks/ckpt_attn/SRC.Field\",\"wb\")as f:\n",
    "     pickle.dump(SRC,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmdvYlZT4fZS"
   },
   "source": [
    "### Transformer model - Implementation\n",
    "\n",
    "[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) module implements the entire encoder-decoder Transformer model, excluding the input token embeddings, position encodings, source and target masks. This module takes in the following parameters:\n",
    "- **$d_{\\text{model}}$** – the number of expected features in the encoder/decoder inputs \n",
    "- **nhead** – the number of heads in the multiheadattention models (the term 'h' in theory)\n",
    "- **num_encoder_layers** - the number of sub-encoder-layers in the encoder\n",
    "- **num_decoder_layers** - the number of sub-decoder-layers in the decoder\n",
    "- **dim_feedforward** -  the dimension of the feedforward network model\n",
    "- **dropout** - the dropout value\n",
    "\n",
    "Let's understand `nn.Transformer` module by passing a sample batch.\n",
    "\n",
    "Let's first convert the src and target word indices to word embeddings:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkn5j2H5ZPdt",
    "outputId": "02022d8a-0413-4225-b91a-a9768c69df48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding_encoder =  Embedding(6461, 512)\n",
      "token_embedding_decoder =  Embedding(5893, 512)\n",
      "src shape =  torch.Size([20, 16])\n",
      "targ shape =  torch.Size([21, 16])\n",
      "src_token_embeddings shape =  torch.Size([20, 16, 512])\n",
      "targ_token_embeddings shape =  torch.Size([21, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "# word embedding layers for encoder (src) and decoder (targ)\n",
    "token_embedding_encoder = nn.Embedding(num_embeddings=len(SRC.vocab), embedding_dim=512).to(device) # src vocab size x embedding size\n",
    "print(\"token_embedding_encoder = \", token_embedding_encoder)\n",
    "token_embedding_decoder = nn.Embedding(num_embeddings=len(TRG.vocab), embedding_dim=512).to(device) # targ vocab size x embedding size\n",
    "print(\"token_embedding_decoder = \", token_embedding_decoder)\n",
    "\n",
    "# print the shape tensors in sample batch: src and targ\n",
    "print(\"src shape = \", src.shape) # src. max. seq len x batch size\n",
    "print(\"targ shape = \", trg.shape) # targ. max. seq len x batch size\n",
    "\n",
    "# pass the word indices to embedding layer to get embeddings\n",
    "src_token_embeddings = token_embedding_encoder(src.to(device))\n",
    "print(\"src_token_embeddings shape = \", src_token_embeddings.shape) # src. max. seq len x batch size x embedding size\n",
    "targ_token_embeddings = token_embedding_decoder(trg.to(device))\n",
    "print(\"targ_token_embeddings shape = \", targ_token_embeddings.shape) # targ. max. seq len x batch size x embedding size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-PQXPReav6B"
   },
   "source": [
    "Having built word embeddings, we can now build position embeddings for both encoder and decoder. Let's start by defining the position embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oG9wCQw7a9k1",
    "outputId": "f329b60c-fb45-4ae8-f6b9-0060be39c7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_embedding_encoder =  Embedding(200, 512)\n",
      "position_embedding_decoder =  Embedding(200, 512)\n"
     ]
    }
   ],
   "source": [
    "# position embedding layers for encoder (src) and decoder (targ)\n",
    "maximum_sentence_len = 200 # this will be the maximum length of source and target sentence that our model can process (can have separate maximum lengths for both encoder and decoder)\n",
    "position_embedding_encoder = nn.Embedding(num_embeddings=maximum_sentence_len, embedding_dim=512).to(device) # maximum_sentence_len x embedding size\n",
    "print(\"position_embedding_encoder = \", position_embedding_encoder)\n",
    "position_embedding_decoder = nn.Embedding(num_embeddings=maximum_sentence_len, embedding_dim=512).to(device) # maximum_sentence_len x embedding size\n",
    "print(\"position_embedding_decoder = \", position_embedding_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFeSYt3vb95Y"
   },
   "source": [
    "Now we can create the positional input for both encoder and decoder. For a single example setting, the positional inputs can be created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWXDi_Khcso6",
    "outputId": "fc9bdf70-3a6e-4851-8e66-bea56d73d8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional input for a single source example =  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19]) torch.Size([20])\n",
      "positional input for a single target example =  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20]) torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# if the no. of tokens in the source sentence is `src_seq_len`\n",
    "src_seq_len = src.size(0)\n",
    "src_position = torch.arange(0, src_seq_len)\n",
    "print('positional input for a single source example = ', src_position, src_position.shape)\n",
    "\n",
    "# if the no. of tokens in the target sentence is `trg_seq_len`\n",
    "trg_seq_len = trg.size(0)\n",
    "trg_position = torch.arange(0, trg_seq_len)\n",
    "print('positional input for a single target example = ', trg_position, trg_position.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3ymxRBUdVbF"
   },
   "source": [
    "If we have a batch of examples, the positional inputs can be extended to batch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUSR0Vzld9gQ",
    "outputId": "a013b81d-7408-43cf-ac97-82ec29d670a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional input for a source batch =  tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
      "        [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "        [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
      "        [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
      "        [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
      "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
      "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
      "        [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16],\n",
      "        [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "        [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18],\n",
      "        [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      "       device='cuda:0') torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = src.size(1) # size of the current batch\n",
    "src_position = (torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, batch_size).to(device))\n",
    "print('positional input for a source batch = ', src_position, src_position.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn33_TdQeW1d"
   },
   "source": [
    "Similarly for the target batch,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BHjO8lfebzF",
    "outputId": "14066aac-caea-4c84-904e-5fcf97cf4ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional input for a target batch =  tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
      "        [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "        [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
      "        [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
      "        [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
      "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
      "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
      "        [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16],\n",
      "        [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "        [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18],\n",
      "        [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19],\n",
      "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]],\n",
      "       device='cuda:0') torch.Size([21, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = trg.size(1) # size of the current batch\n",
    "targ_position = (torch.arange(0, trg_seq_len).unsqueeze(1).expand(trg_seq_len, batch_size).to(device))\n",
    "print('positional input for a target batch = ', targ_position, targ_position.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l638opIefT9_"
   },
   "source": [
    "The positional embeddings can be obtained by passing the position input to position embeddings layer of encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6U-EFvc9f4pt",
    "outputId": "a76eec98-f7eb-4750-e7d1-9bf4c084569f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional input for source =  torch.Size([20, 16])\n",
      "positional embeddings for source =  torch.Size([20, 16, 512])\n",
      "positional input for target =  torch.Size([21, 16])\n",
      "positional embeddings for target =  torch.Size([21, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src_pos_embeddings = position_embedding_encoder(src_position)\n",
    "print('positional input for source = ', src_position.shape)\n",
    "print('positional embeddings for source = ', src_pos_embeddings.shape)\n",
    "\n",
    "targ_pos_embeddings = position_embedding_encoder(targ_position)\n",
    "print('positional input for target = ', targ_position.shape)\n",
    "print('positional embeddings for target = ', targ_pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "183LmdZNever"
   },
   "source": [
    "Having built word embedding and position embedding, we can now add both the embeddings to create the input embedding to Transformer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sYHC_Ype4Uy",
    "outputId": "ff69b567-2519-4b28-a4fe-821f448b4249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embedding for source =  torch.Size([20, 16, 512])\n",
      "input embedding for target =  torch.Size([21, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src_input_embedding = src_token_embeddings + src_pos_embeddings\n",
    "print('input embedding for source = ', src_input_embedding.shape)\n",
    "targ_input_embedding = targ_token_embeddings + targ_pos_embeddings\n",
    "print('input embedding for target = ', targ_input_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGWNRwvKh5kx"
   },
   "source": [
    "We can optionally add dropout to both the input embeddings before feeding as an input to Transformer.\n",
    "\n",
    "We need to create a binary mask for the source tokens that marks the pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVpy0OHPihch",
    "outputId": "a6508b99-0c2a-4e64-c65c-e02daa8b9ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# create the source (pad) mask\n",
    "mask_src = (src == SRC.vocab.stoi[SRC.pad_token]).transpose(0, 1).to(device) # batch size x src max. seq length\n",
    "print(mask_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmjynIoDj0nG"
   },
   "source": [
    "We can now create an instance from `nn.Transformer` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhGWDYg2kMnu",
    "outputId": "c70de710-c88a-4bc9-f2e9-e876afee4872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check doc for details about the input arguments\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "embed_dim = 512 #  the number of expected features in the encoder/decoder inputs\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "num_encoder_layers = 2 # the number of sub-encoder-layers in the encoder\n",
    "num_decoder_layers = 2 # the number of sub-decoder-layers in the decoder\n",
    "dim_feedforward = 1024 # the dimension of the feedforward network model\n",
    "dropout = 0.1 # the dropout value (default=0.1)\n",
    "transformer_layer = nn.Transformer(d_model=embed_dim, nhead=nhead, num_encoder_layers=num_encoder_layers, dropout=dropout, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "print(transformer_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLW-JdWmjSXe"
   },
   "source": [
    "We have created all the inputs that we need to pass to Transformer module except one: **target mask** that makes the decoder attend to only the past decoder context while predicting the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af2LisC3jRuo",
    "outputId": "b5c65efc-6c32-4139-a073-cadbb8f01f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask target =  tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0') torch.Size([21, 21])\n"
     ]
    }
   ],
   "source": [
    "# create the target (self-attention) mask \n",
    "targ_seq_len = trg.size(0)\n",
    "mask_targ = transformer_layer.generate_square_subsequent_mask(targ_seq_len).to(device)\n",
    "print(\"mask target = \", mask_targ, mask_targ.shape)  # targ. seq len x targ. seq len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqZzGkufqf9q"
   },
   "source": [
    "Let's now pass all the tensor we've created to transformer layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38ivfxVuqjlX",
    "outputId": "b33cdcec-3230-4e68-b25a-6daedbf16480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of src_input_embedding =  torch.Size([20, 16, 512])\n",
      "shape of targ_input_embedding =  torch.Size([21, 16, 512])\n",
      "shape of mask_src =  torch.Size([16, 20])\n",
      "shape of mask_targ =  torch.Size([21, 21])\n",
      "shape of the transformer output =  torch.Size([21, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of src_input_embedding = \", src_input_embedding.shape) # src. seq len x batch size x embedding size \n",
    "print(\"shape of targ_input_embedding = \", targ_input_embedding.shape)  # targ. seq len x batch size x embedding size \n",
    "print(\"shape of mask_src = \", mask_src.shape) #  batch size x src. seq len\n",
    "print(\"shape of mask_targ = \", mask_targ.shape) # targ. seq len x targ. seq len\n",
    "output = transformer_layer(src_input_embedding, targ_input_embedding, src_key_padding_mask=mask_src, tgt_mask=mask_targ)\n",
    "print(\"shape of the transformer output = \", output.shape) # targ. seq len x batch size x embedding size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qzsqImyYvd4"
   },
   "source": [
    "Having seen an example for using Transformer module, let's implement the Transformer model for machine translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lzTQKNQ9d6gw"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, src_vocab, trg_vocab, embed_dim, nhead, num_encoder_layers, dropout, num_decoder_layers, dim_feedforward, maximum_sentence_len=200):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    # get initial hyper-parameters\n",
    "    self.src_vocab = src_vocab\n",
    "    self.trg_vocab = trg_vocab\n",
    "    self.embed_dim = embed_dim\n",
    "    self.nhead = nhead\n",
    "    self.num_encoder_layers = num_encoder_layers\n",
    "    self.dropout = dropout\n",
    "    self.num_decoder_layers = num_decoder_layers\n",
    "    self.dim_feedforward = dim_feedforward\n",
    "\n",
    "    # add embedding layers\n",
    "    self.token_embedding_encoder = nn.Embedding(num_embeddings=self.src_vocab, embedding_dim=self.embed_dim)\n",
    "    self.token_embedding_decoder = nn.Embedding(num_embeddings=self.trg_vocab, embedding_dim=self.embed_dim)\n",
    "    self.position_embedding_encoder = nn.Embedding(num_embeddings=maximum_sentence_len, embedding_dim=self.embed_dim)\n",
    "    self.position_embedding_decoder = nn.Embedding(num_embeddings=maximum_sentence_len, embedding_dim=self.embed_dim)\n",
    "    \n",
    "    # Encoder-Decoder Transformer\n",
    "    self.transformer = nn.Transformer(d_model=self.embed_dim, nhead=self.nhead, num_encoder_layers=self.num_encoder_layers, dropout=self.dropout, num_decoder_layers=self.num_decoder_layers, dim_feedforward=self.dim_feedforward)\n",
    "    \n",
    "    # output layer to predict next token\n",
    "    self.decoder = nn.Linear(self.embed_dim, self.trg_vocab)\n",
    "    self.drop_layer = nn.Dropout()\n",
    "\n",
    "  def forward(self, src, tgr):\n",
    "    # read shapes\n",
    "    # src = src_seq_len x batch_size\n",
    "    # tgr = targ_seq_len x batch_size\n",
    "    src_seq_len, batch_size = src.shape\n",
    "    targ_seq_len, _ = tgr.shape\n",
    "\n",
    "    # create position input for encoder\n",
    "    src_position = (torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, batch_size).to(device))\n",
    "    # src_position = src_seq_len x batch_size\n",
    "\n",
    "    # create position input for decoder\n",
    "    targ_position = (torch.arange(0, targ_seq_len).unsqueeze(1).expand(targ_seq_len, batch_size).to(device))\n",
    "    # src_position = targ_seq_len x batch_size\n",
    "\n",
    "    # input embedding by merging token embedding with position embedding\n",
    "    embed_src = self.drop_layer((self.token_embedding_encoder(src) + self.position_embedding_encoder(src_position)))\n",
    "    embed_tgr = self.drop_layer((self.token_embedding_decoder(tgr) + self.position_embedding_decoder(targ_position)))\n",
    "    # embed_src = src_seq_len x batch_size x d_model\n",
    "    # embed_src = targ_seq_len x batch_size x d_model\n",
    "\n",
    "    # create mask for source\n",
    "    mask_src = (src == SRC.vocab.stoi[SRC.pad_token]).transpose(0, 1).to(device)\n",
    "    # mask_src = batch_size x src_seq_len\n",
    "\n",
    "    # create mask for target\n",
    "    mask_targ = self.transformer.generate_square_subsequent_mask(targ_seq_len).to(device)\n",
    "    # mask_targ = targ_seq_len x targ_seq_len\n",
    "\n",
    "    # feed via transformer\n",
    "    output = self.transformer(embed_src, embed_tgr, src_key_padding_mask=mask_src, tgt_mask=mask_targ)\n",
    "    # output = targ_seq_len x batch_size x d_model\n",
    "\n",
    "    # transform the output to match no of. tokens in target vocab \n",
    "    output = self.decoder(output) \n",
    "    # output = targ_seq_len x batch_size x targ_vocab_size\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoS8rucEedqT"
   },
   "source": [
    "Let's set the hyperparameters and create a model instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7WodGURxfLeW"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "src_vocab = len(SRC.vocab)\n",
    "trg_vocab = len(TRG.vocab)\n",
    "embed_dim = 512\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dropout = 0.1 \n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 512\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# model instance\n",
    "model = Transformer(src_vocab, trg_vocab, embed_dim, nhead, num_encoder_layers, dropout, num_decoder_layers, dim_feedforward).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtQFCFbOfSi6"
   },
   "source": [
    "Let's define the train logic (for a single epoch), which is very similar to previous seq2seq tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "PViJMT42fd13"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC.to(device)\n",
    "        trg = batch.TRG.to(device)\n",
    "        # src = src seq len x batch size\n",
    "        # trg = targ seq len x batch size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:-1, :]) # for target, provide targ seq len-1 tokens for each sentence\n",
    "        \n",
    "        #output = [targ seq len-1, batch size, output dim]\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = trg[1:].reshape(-1)\n",
    "\n",
    "        # loss function works only 2d logits, 1d targets\n",
    "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "        # target shape shape should be [(targ seq len - 1) * batch_size]\n",
    "        # output shape should be [(targ seq len - 1) * batch_size, output_dim]\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru4U9cK4fo6B"
   },
   "source": [
    "Let's define the inference logic for evaluating the quality of the model based on BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BMTmcVhefrWH"
   },
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention = False, max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.legacy.data import TabularDataset\n",
    "    from torchtext.legacy.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "      dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "      sort = False, \n",
    "      batch_size=1,\n",
    "      sort_key=None,\n",
    "      shuffle=False,\n",
    "      sort_within_batch=False,\n",
    "      device = device,\n",
    "      train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_gold_trg_tokids = []\n",
    "    all_translated_trg_tokids = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC.to(device)\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG.to(device)\n",
    "\n",
    "            #src = GOLD_SRC.to(device)\n",
    "            #trg = GOLD_TRG.to(device)\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            outputs = [trg_vocab.vocab.stoi[\"<sos>\"]]\n",
    "            for i in range(max_trg_len):\n",
    "                trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "                \n",
    "                output = model(src, trg_tensor)\n",
    "\n",
    "                topv, topi = output[-1,0,:].topk(1)\n",
    "                cur_decoded_token = topi.squeeze().detach()  # detach from history as input\n",
    "                outputs.append(cur_decoded_token.item())\n",
    "\n",
    "                if cur_decoded_token.item() == trg_vocab.vocab.stoi[\"<eos>\"]:\n",
    "                    break\n",
    "            all_translated_trg_tokids.append(outputs[1:-1])\n",
    "            all_gold_trg_tokids.append([ trg[idx, 0].item() for idx in range(1, trg.size(0)-1)])\n",
    "    \n",
    "    # convert token ids to token strs\n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_gold_trg_tokids)): \n",
    "        all_gold_text.append([[trg_vocab.vocab.itos[idx] for idx in all_gold_trg_tokids[i]]])\n",
    "        all_translated_text.append([trg_vocab.vocab.itos[idx] for idx in all_translated_trg_tokids[i]])\n",
    "        \n",
    "    corpus_bleu_score = corpus_bleu(all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKl8mukAgC2g"
   },
   "source": [
    "Let's define the evaluation logic to compute the quality of the model based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XXny41OTgSvR"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC.to(device)\n",
    "            trg = batch.TRG.to(device)\n",
    "\n",
    "            output = model(src, trg[:-1, :]) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            #output = output[1:].view(-1, output_dim)\n",
    "            #trg = trg[1:].view(-1)\n",
    "\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            target = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #break\n",
    "        \n",
    "    bleu = inference(model, \"./drive/MyDrive/Colab Notebooks/eng-fre/val_eng_fre.tsv\", SRC, TRG, False, 64)\n",
    "    return epoch_loss / len(iterator) , bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3dXPDyxglqh"
   },
   "source": [
    "Let's perform the full-training of the Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSpD9QFZgp4Q",
    "outputId": "5ac2a15f-055f-4113-b95b-289483002cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n",
      "Training started...\n",
      "Epoch: 01 | Time: 1m 43s\n",
      "\t Train Loss: 4.014 | Train PPL:  55.392\n",
      "\t Val. Loss: 3.014 |  Val. PPL:  20.364\n",
      "\t Val. BLEU:   0.148\n",
      "Epoch: 02 | Time: 1m 33s\n",
      "\t Train Loss: 3.119 | Train PPL:  22.618\n",
      "\t Val. Loss: 2.544 |  Val. PPL:  12.733\n",
      "\t Val. BLEU:   0.236\n",
      "Epoch: 03 | Time: 1m 33s\n",
      "\t Train Loss: 2.732 | Train PPL:  15.369\n",
      "\t Val. Loss: 2.247 |  Val. PPL:   9.459\n",
      "\t Val. BLEU:   0.292\n",
      "Epoch: 04 | Time: 1m 36s\n",
      "\t Train Loss: 2.456 | Train PPL:  11.662\n",
      "\t Val. Loss: 2.066 |  Val. PPL:   7.894\n",
      "\t Val. BLEU:   0.306\n",
      "Epoch: 05 | Time: 1m 33s\n",
      "\t Train Loss: 2.241 | Train PPL:   9.400\n",
      "\t Val. Loss: 1.920 |  Val. PPL:   6.819\n",
      "\t Val. BLEU:   0.348\n",
      "Epoch: 06 | Time: 1m 32s\n",
      "\t Train Loss: 2.065 | Train PPL:   7.884\n",
      "\t Val. Loss: 1.793 |  Val. PPL:   6.007\n",
      "\t Val. BLEU:   0.369\n",
      "Epoch: 07 | Time: 1m 32s\n",
      "\t Train Loss: 1.922 | Train PPL:   6.833\n",
      "\t Val. Loss: 1.690 |  Val. PPL:   5.420\n",
      "\t Val. BLEU:   0.393\n",
      "Epoch: 08 | Time: 1m 35s\n",
      "\t Train Loss: 1.799 | Train PPL:   6.045\n",
      "\t Val. Loss: 1.608 |  Val. PPL:   4.995\n",
      "\t Val. BLEU:   0.415\n",
      "Epoch: 09 | Time: 1m 42s\n",
      "\t Train Loss: 1.695 | Train PPL:   5.448\n",
      "\t Val. Loss: 1.574 |  Val. PPL:   4.824\n",
      "\t Val. BLEU:   0.427\n",
      "Epoch: 10 | Time: 1m 44s\n",
      "\t Train Loss: 1.604 | Train PPL:   4.972\n",
      "\t Val. Loss: 1.515 |  Val. PPL:   4.550\n",
      "\t Val. BLEU:   0.438\n",
      "Epoch: 11 | Time: 1m 50s\n",
      "\t Train Loss: 1.522 | Train PPL:   4.580\n",
      "\t Val. Loss: 1.498 |  Val. PPL:   4.473\n",
      "\t Val. BLEU:   0.446\n",
      "Epoch: 12 | Time: 1m 50s\n",
      "\t Train Loss: 1.452 | Train PPL:   4.271\n",
      "\t Val. Loss: 1.460 |  Val. PPL:   4.305\n",
      "\t Val. BLEU:   0.451\n",
      "Epoch: 13 | Time: 1m 49s\n",
      "\t Train Loss: 1.386 | Train PPL:   3.998\n",
      "\t Val. Loss: 1.437 |  Val. PPL:   4.209\n",
      "\t Val. BLEU:   0.454\n",
      "Epoch: 14 | Time: 1m 49s\n",
      "\t Train Loss: 1.327 | Train PPL:   3.770\n",
      "\t Val. Loss: 1.409 |  Val. PPL:   4.092\n",
      "\t Val. BLEU:   0.463\n",
      "Epoch: 15 | Time: 1m 48s\n",
      "\t Train Loss: 1.272 | Train PPL:   3.569\n",
      "\t Val. Loss: 1.395 |  Val. PPL:   4.036\n",
      "\t Val. BLEU:   0.469\n",
      "\t Test BLEU:   0.469\n"
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# set the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# create the loss function\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ', TRG_PAD_IDX)\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# numer of epochs (hyperparameter)\n",
    "N_EPOCHS = 15\n",
    "\n",
    "# initial best valid loss\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# kick-start training\n",
    "print('Training started...')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, bleu = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"./drive/MyDrive/Colab Notebooks/ckpt_ex1/seq2seq_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\t Val. BLEU: {bleu:7.3f}')\n",
    "\n",
    "# Evaluate the model after the last epoch on the test set\n",
    "test_loss, bleu_test = evaluate(model, test_iter, criterion)\n",
    "print(f'\\t Test BLEU: {bleu_test:7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkhxPPcxhaMO"
   },
   "source": [
    "## Reference:\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "\n",
    "https://pytorch.org/docs/master/nn.html#transformerencoderlayer\n",
    "\n",
    "https://pytorch.org/docs/master/_modules/torch/nn/modules/transformer.html#Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3mTYiao6ha4P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
